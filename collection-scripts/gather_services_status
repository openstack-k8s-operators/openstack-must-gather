#!/bin/bash

# When called from the shell directly
if [[ -z "$DIR_NAME" ]]; then
    CALLED=1
    DIR_NAME=$( cd -- "$( dirname -- "${BASH_SOURCE[0]}" )" &> /dev/null && pwd )
    source "${DIR_NAME}/common.sh"
fi

# shellcheck disable=SC2139  # We want it expanded when defined
alias os="/usr/bin/oc -n ${OSP_NS} rsh openstackclient openstack "

# For each service passed an input, if the associated entry exists,
# we can call the related function that processes specific service
# commands
get_status() {
    service="$1"
    echo "Gather ctlplane service info: $service"
    case "${service}" in
    "openstack")
        get_openstack_status
        ;;
    "manila")
        get_manila_status
        ;;
    "neutron")
        get_neutron_status
        ;;
    "cinder")
        get_cinder_status
        ;;
    "heat")
        get_heat_status
        ;;
    "nova")
        get_nova_status
        ;;
    "placement")
        get_placement_status
        ;;
    "ironic")
        get_ironic_status
        ;;
    "aodh")
        get_aodh_status
        ;;
    "ceilometer")
        get_ceilometer_status
        ;;
    "octavia")
        get_octavia_status
        ;;
    "glance")
        get_glance_status
        ;;
    "designate")
        get_designate_status
        ;;
    "rabbitmq")
        get_rabbitmq_status
        ;;
    *) ;;
    esac
}

# Generic OpenStack cltplane gathering -
get_openstack_status() {
    mkdir -p "$BASE_COLLECTION_PATH"/ctlplane
    run_bg ${BASH_ALIASES[os]} endpoint list '>' "$BASE_COLLECTION_PATH"/ctlplane/endpoints
    run_bg ${BASH_ALIASES[os]} service list '>' "$BASE_COLLECTION_PATH"/ctlplane/services
    run_bg ${BASH_ALIASES[os]} network agent list '>' "$BASE_COLLECTION_PATH"/ctlplane/network_agent_list
}

# Rabbitmq info gathering -
get_rabbitmq_status() {
    local RABBIT_PATH="$BASE_COLLECTION_PATH/ctlplane/rabbitmq"
    mkdir -p "$RABBIT_PATH"
    rabbit_instances=$(oc -n "${OSP_NS}" get pods -l "$RABBITMQ_SELECTOR" -o custom-columns=NAME:.metadata.name --no-headers)
    for inst in $rabbit_instances; do
        mkdir -p "$RABBIT_PATH/$inst"
        run_bg oc -n "${OSP_NS}" rsh "$inst" rabbitmqctl status '>' "$RABBIT_PATH"/"$inst"/status
        run_bg oc -n "${OSP_NS}" rsh "$inst" rabbitmqctl cluster_status '>' "$RABBIT_PATH"/"$inst"/cluster_status
        run_bg oc -n "${OSP_NS}" rsh "$inst" rabbitmqctl list_queues '>' "$RABBIT_PATH"/"$inst"/list_queues
        run_bg oc -n "${OSP_NS}" rsh "$inst" rabbitmqctl list_connections '>' "$RABBIT_PATH"/"$inst"/list_connections
        run_bg oc -n "${OSP_NS}" rsh "$inst" rabbitmqctl list_policies '>' "$RABBIT_PATH"/"$inst"/list_policies
        run_bg oc -n "${OSP_NS}" rsh "$inst" rabbitmqctl list_unresponsive_queues '>' "$RABBIT_PATH"/"$inst"/list_unresponsive_queues
    done
}

# Manila service gathering -
get_manila_status() {
    local MANILA_PATH="$BASE_COLLECTION_PATH/ctlplane/manila"
    mkdir -p "$MANILA_PATH"
    run_bg ${BASH_ALIASES[os]} share service list '>' "$MANILA_PATH"/service_list
    run_bg ${BASH_ALIASES[os]} share type list '>' "$MANILA_PATH"/share_types
    run_bg ${BASH_ALIASES[os]} share pool list --detail '>' "$MANILA_PATH"/pool_list
}

# Neutron service gathering -
get_neutron_status() {
    local NEUTRON_PATH="$BASE_COLLECTION_PATH/ctlplane/neutron"
    mkdir -p "$NEUTRON_PATH"
    run_bg ${BASH_ALIASES[os]} subnet list '>' "$NEUTRON_PATH"/subnet_list
    run_bg ${BASH_ALIASES[os]} port list '>' "$NEUTRON_PATH"/port_list
    run_bg ${BASH_ALIASES[os]} router list '>' "$NEUTRON_PATH"/router_list
    run_bg ${BASH_ALIASES[os]} network agent list '>' "$NEUTRON_PATH"/agent_list
    run_bg ${BASH_ALIASES[os]} network list '>' "$NEUTRON_PATH"/network_list
    run_bg ${BASH_ALIASES[os]} extension list '>' "$NEUTRON_PATH"/extension_list
    run_bg ${BASH_ALIASES[os]} floating ip list '>' "$NEUTRON_PATH"/floating_ip_list
    run_bg ${BASH_ALIASES[os]} security group list '>' "$NEUTRON_PATH"/security_group_list
}

# Cinder service gathering - services, vol types, qos, transfers, pools,
get_cinder_status() {
    local CINDER_PATH="$BASE_COLLECTION_PATH/ctlplane/cinder"
    mkdir -p "$CINDER_PATH"
    run_bg ${BASH_ALIASES[os]} volume service list '>' "$CINDER_PATH"/service_list
    run_bg ${BASH_ALIASES[os]} volume type list --long '>' "$CINDER_PATH"/type_list
    run_bg ${BASH_ALIASES[os]} volume qos list '>' "$CINDER_PATH"/qos_list
    run_bg ${BASH_ALIASES[os]} volume transfer request list --all-project '>' "$CINDER_PATH"/transfer_list
    run_bg ${BASH_ALIASES[os]} --os-volume-api-version 3.12 volume summary --all-projects '>' "$CINDER_PATH"/total_volumes_list
    # Add --fit once we have https://review.opendev.org/c/openstack/python-openstackclient/+/895971
    run_bg ${BASH_ALIASES[os]} volume backend pool list --long '>' "$CINDER_PATH"/pool_list
}

# Heat service gathering - services
get_heat_status() {
    local HEAT_PATH="$BASE_COLLECTION_PATH/ctlplane/heat"
    mkdir -p "$HEAT_PATH"
    ${BASH_ALIASES[os]} orchestration service list > "$HEAT_PATH"/service_list
}

# Nova service gathering - sevices, hypervisors, cells, host mappings, allocation audit
get_nova_status() {
    local NOVA_PATH="$BASE_COLLECTION_PATH/ctlplane/nova"
    mkdir -p "$NOVA_PATH"
    run_bg ${BASH_ALIASES[os]} compute service list '>' "$NOVA_PATH"/service_list
    run_bg ${BASH_ALIASES[os]} hypervisor list '>' "$NOVA_PATH"/hypervisor_list
    run_bg /usr/bin/oc -n ${OSP_NS} exec -t nova-cell0-conductor-0 -- nova-manage cell_v2 list_cells '>' "$NOVA_PATH"/cell_list
    run_bg /usr/bin/oc -n ${OSP_NS} exec -t nova-cell0-conductor-0 -- nova-manage cell_v2 list_hosts '>' "$NOVA_PATH"/host_list
    run_bg ${BASH_ALIASES[os]} aggregate list --long '>' "$NOVA_PATH"/aggregate_list
}

# Placement service gathering - capacity overview
get_placement_status() {
    local PLACEMENT_PATH="$BASE_COLLECTION_PATH/ctlplane/placement"
    mkdir -p "$PLACEMENT_PATH"
    # NOTE(gibi): this gives us a very simple resource capacity view of the
    # cluster. It is intentionally uses 1 MB RAM query to get one candidate
    # from each compute
    run_bg ${BASH_ALIASES[os]} allocation candidate list --resource MEMORY_MB=1 --max-width 200 -c "'resource provider'" -c "'inventory used/capacity'" -c "traits" '>' "$PLACEMENT_PATH"/allocation_candidate_list
    run_bg ${BASH_ALIASES[os]} resource class list '>' "$PLACEMENT_PATH"/resource_class_list
    run_bg ${BASH_ALIASES[os]} trait list '>' "$PLACEMENT_PATH"/trait_list
}

# Ironic service gathering - nodes, ports, conductors if we can get them.
get_ironic_status() {
    local IRONIC_PATH="$BASE_COLLECTION_PATH/ctlplane/ironic"
    mkdir -p "$IRONIC_PATH"
    # NOTE(TheJulia): The idea here is to try and collect information visible,
    # as Ironic has filtering in place on all project scoped requests,
    # as agreed by the Ironic community.
    run_bg ${BASH_ALIASES[os]} baremetal node list --long '>' "$IRONIC_PATH"/node_list
    run_bg ${BASH_ALIASES[os]} baremetal port list --long '>' "$IRONIC_PATH"/port_list
    run_bg ${BASH_ALIASES[os]} baremetal port group list --long '>' "$IRONIC_PATH"/port_group_list
    run_bg ${BASH_ALIASES[os]} baremetal volume connector list --long '>' "$IRONIC_PATH"/volume_connector_list
    run_bg ${BASH_ALIASES[os]} baremetal volume target list --long '>' "$IRONIC_PATH"/volume_target_list
    run_bg ${BASH_ALIASES[os]} baremetal allocation list --long '>' "$IRONIC_PATH"/allocation_list
    # Driver/Conductor lists are restricted endpoints by default since
    # they provide insight into the overall infrastucture which would
    # be inappropriate in a public cloud context and we don't inherently
    # have the required elevated rights "out of the" box to gather it.
}

# Aodh service gathering - alarms
get_aodh_status() {
    local AODH_PATH="$BASE_COLLECTION_PATH/ctlplane/aodh"
    mkdir -p "$AODH_PATH"
    run_bg ${BASH_ALIASES[os]} alarm list '>' "$AODH_PATH"/alarm_list
}

# Ceilometer, sg-core, prometheus service gathering - metrics
get_ceilometer_status() {
    local CEILOMETER_PATH="$BASE_COLLECTION_PATH/ctlplane/ceilometer"
    if /usr/bin/oc -n ${OSP_NS} get metricstorage metric-storage &> /dev/null; then
        # For `openstack metric list` command to work we need ceilometer
        # in the openstack as well as metricstorage deployed
        # on openshift.
        mkdir -p "$CEILOMETER_PATH"
        run_bg ${BASH_ALIASES[os]} metric list --disable-rbac '>' "$CEILOMETER_PATH"/metric_list
    fi
}

# Octavia service gathering - loadbalancers,
get_octavia_status() {
    local OCTAVIA_PATH="$BASE_COLLECTION_PATH/ctlplane/octavia"
    mkdir -p "$OCTAVIA_PATH"
    resources="amphora availabilityzone availabilityzoneprofile flavor flavorprofile healthmonitor l7policy "
    resources="$resources listener pool provider quota"

    for r in $resources; do
        run_bg ${BASH_ALIASES[os]} loadbalancer $r list '>' "$OCTAVIA_PATH"/"${r}_list"
    done;

    run_bg ${BASH_ALIASES[os]} loadbalancer provider list '>' "$OCTAVIA_PATH"/provider_list
}

# Glance service gathering - task
get_glance_status() {
    local GLANCE_PATH="$BASE_COLLECTION_PATH/ctlplane/glance"
    mkdir -p "$GLANCE_PATH"
    run_bg ${BASH_ALIASES[os]} image task list '>' "$GLANCE_PATH"/task_list
}

# Designate service gathering - dns resources,
get_designate_status() {
    local DESIGNATE_PATH="$BASE_COLLECTION_PATH/ctlplane/designate"
    mkdir -p "$DESIGNATE_PATH"

    run_bg ${BASH_ALIASES[os]} zone list '>' "$DESIGNATE_PATH"/zone_list
    run_bg ${BASH_ALIASES[os]} dns quota list '>' "$DESIGNATE_PATH"/dns_quota_list
    run_bg ${BASH_ALIASES[os]} ptr record list '>' "$DESIGNATE_PATH"/ptr_record_list
    run_bg ${BASH_ALIASES[os]} zone export list '>' "$DESIGNATE_PATH"/zone_export_list
    run_bg ${BASH_ALIASES[os]} zone import list '>' "$DESIGNATE_PATH"/zone_import_list
    run_bg ${BASH_ALIASES[os]} zone transfer request list '>' "$DESIGNATE_PATH"/transfer_request_list

    ZONES=$(run_bg ${BASH_ALIASES[os]} zone list -f value -c id)
    for z in ${ZONES}; do
        run_bg ${BASH_ALIASES[os]} recordset list ${z} '>>' "$DESIGNATE_PATH"/recordset_list
        run_bg ${BASH_ALIASES[os]} zone share list ${z} '>>' "$DESIGNATE_PATH"/zone_share_list
    done

    WORKER=$(oc get pods -l component=designate-worker -o custom-columns=NAME:.metadata.name --no-headers | head -n 1)
    # We can add the --all_pools flag when it is available in openstackclient
    run_bg /usr/bin/oc -n ${OSP_NS} exec -t ${WORKER} -- designate-manage pool show_config '>' "$DESIGNATE_PATH"/pool_list
}

# first we gather generic status of the openstack ctlplane
# then we process the existing services (if an associated
# function has been defined)
run_bg get_status "openstack"

# we gather rabbitmq status in advance before moving to the
# dynamically generated list of services
run_bg get_status "rabbitmq"

# get the list of existing ctlplane services (once) and
# filter the whole list processing only services with an
# associated function
services=$(${BASH_ALIASES[os]} service list -c Name -f value)
for svc in "${OSP_SERVICES[@]}"; do
    [[ "${services[*]}" =~ ${svc} ]] && get_status "$svc"
done

[[ $CALLED -eq 1 ]] && wait_bg
