#!/bin/bash

# When called from the shell directly
if [[ -z "$DIR_NAME" ]]; then
    CALLED=1
    DIR_NAME=$( cd -- "$( dirname -- "${BASH_SOURCE[0]}" )" &> /dev/null && pwd )
    source "${DIR_NAME}/common.sh"
fi

alias os="/usr/bin/oc -n openstack rsh openstackclient openstack "

# For each service passed an input, if the associated entry exists,
# we can call the related function that processes specific service
# commands
get_status() {
    service="$1"
    echo "Gather ctlplane service info: $service"
    case "${service}" in
    "openstack")
        get_openstack_status
        ;;
    "manila")
        get_manila_status
        ;;
    "neutron")
        get_neutron_status
        ;;
    "cinder")
        get_cinder_status
        ;;
    "heat")
        get_heat_status
        ;;
    "nova")
        get_nova_status
        ;;
    "placement")
        get_placement_status
        ;;
    "ironic")
        get_ironic_status
        ;;
    "aodh")
        get_aodh_status
        ;;
    "ceilometer")
        get_ceilometer_status
        ;;
    *) ;;
    esac
}

# Generic OpenStack cltplane gathering -
get_openstack_status() {
    mkdir -p "$BASE_COLLECTION_PATH"/ctlplane
    run_bg ${BASH_ALIASES[os]} endpoint list '>' "$BASE_COLLECTION_PATH"/ctlplane/endpoints
    run_bg ${BASH_ALIASES[os]} service list '>' "$BASE_COLLECTION_PATH"/ctlplane/services
    run_bg ${BASH_ALIASES[os]} network agent list '>' "$BASE_COLLECTION_PATH"/ctlplane/network_agent_list
}

# Manila service gathering -
get_manila_status() {
    local MANILA_PATH="$BASE_COLLECTION_PATH/ctlplane/manila"
    mkdir -p "$MANILA_PATH"
    run_bg ${BASH_ALIASES[os]} share service list '>' "$MANILA_PATH"/service_list
    run_bg ${BASH_ALIASES[os]} share type list '>' "$MANILA_PATH"/share_types
    run_bg ${BASH_ALIASES[os]} share pool list --detail '>' "$MANILA_PATH"/pool_list
}

# Neutron service gathering -
get_neutron_status() {
    local NEUTRON_PATH="$BASE_COLLECTION_PATH/ctlplane/neutron"
    mkdir -p "$NEUTRON_PATH"
    run_bg ${BASH_ALIASES[os]} subnet list '>' "$NEUTRON_PATH"/subnet_list
    run_bg ${BASH_ALIASES[os]} port list '>' "$NEUTRON_PATH"/port_list
    run_bg ${BASH_ALIASES[os]} router list '>' "$NEUTRON_PATH"/router_list
    run_bg ${BASH_ALIASES[os]} network agent list '>' "$NEUTRON_PATH"/agent_list
    run_bg ${BASH_ALIASES[os]} network list '>' "$NEUTRON_PATH"/network_list
    run_bg ${BASH_ALIASES[os]} extension list '>' "$NEUTRON_PATH"/extension_list
    run_bg ${BASH_ALIASES[os]} floating ip list '>' "$NEUTRON_PATH"/floating_ip_list
    run_bg ${BASH_ALIASES[os]} security group list '>' "$NEUTRON_PATH"/security_group_list
}

# Cinder service gathering - services, vol types, qos, transfers, pools,
get_cinder_status() {
    local CINDER_PATH="$BASE_COLLECTION_PATH/ctlplane/cinder"
    mkdir -p "$CINDER_PATH"
    run_bg ${BASH_ALIASES[os]} volume service list '>' "$CINDER_PATH"/service_list
    run_bg ${BASH_ALIASES[os]} volume type list --long '>' "$CINDER_PATH"/type_list
    run_bg ${BASH_ALIASES[os]} volume qos list '>' "$CINDER_PATH"/qos_list
    run_bg ${BASH_ALIASES[os]} volume transfer request list --all-project '>' "$CINDER_PATH"/transfer_list
    run_bg ${BASH_ALIASES[os]} --os-volume-api-version 3.12 volume summary --all-projects '>' "$CINDER_PATH"/total_volumes_list
    # Add --fit once we have https://review.opendev.org/c/openstack/python-openstackclient/+/895971
    run_bg ${BASH_ALIASES[os]} volume backend pool list --long '>' "$CINDER_PATH"/pool_list
}

# Heat service gathering - services
get_heat_status() {
    local HEAT_PATH="$BASE_COLLECTION_PATH/ctlplane/heat"
    mkdir -p "$HEAT_PATH"
    ${BASH_ALIASES[os]} orchestration service list > "$HEAT_PATH"/service_list
}

# Nova service gathering - sevices, hypervisors, cells, host mappings, allocation audit
get_nova_status() {
    local NOVA_PATH="$BASE_COLLECTION_PATH/ctlplane/nova"
    mkdir -p "$NOVA_PATH"
    run_bg ${BASH_ALIASES[os]} compute service list '>' "$NOVA_PATH"/service_list
    run_bg ${BASH_ALIASES[os]} hypervisor list '>' "$NOVA_PATH"/hypervisor_list
    run_bg /usr/bin/oc -n openstack exec -t nova-cell0-conductor-0 -- nova-manage cell_v2 list_cells '>' "$NOVA_PATH"/cell_list
    run_bg /usr/bin/oc -n openstack exec -t nova-cell0-conductor-0 -- nova-manage cell_v2 list_hosts '>' "$NOVA_PATH"/host_list
    run_bg ${BASH_ALIASES[os]} aggregate list --long '>' "$NOVA_PATH"/aggregate_list
}

# Placement service gathering - capacity overview
get_placement_status() {
    local PLACEMENT_PATH="$BASE_COLLECTION_PATH/ctlplane/placement"
    mkdir -p "$PLACEMENT_PATH"
    # NOTE(gibi): this gives us a very simple resource capacity view of the
    # cluster. It is intentionally uses 1 MB RAM query to get one candidate
    # from each compute
    run_bg ${BASH_ALIASES[os]} allocation candidate list --resource MEMORY_MB=1 --max-width 200 -c "'resource provider'" -c "'inventory used/capacity'" -c "traits" '>' "$PLACEMENT_PATH"/allocation_candidate_list
    run_bg ${BASH_ALIASES[os]} resource class list '>' "$PLACEMENT_PATH"/resource_class_list
    run_bg ${BASH_ALIASES[os]} trait list '>' "$PLACEMENT_PATH"/trait_list
}

# Ironic service gathering - nodes, ports, conductors if we can get them.
get_ironic_status() {
    local IRONIC_PATH="$BASE_COLLECTION_PATH/ctlplane/ironic"
    mkdir -p "$IRONIC_PATH"
    # NOTE(TheJulia): The idea here is to try and collect information visible,
    # as Ironic has filtering in place on all project scoped requests,
    # as agreed by the Ironic community.
    run_bg ${BASH_ALIASES[os]} baremetal node list --long '>' "$IRONIC_PATH"/node_list
    run_bg ${BASH_ALIASES[os]} baremetal port list --long '>' "$IRONIC_PATH"/port_list
    run_bg ${BASH_ALIASES[os]} baremetal portgroup list --long '>' "$IRONIC_PATH"/portgroup_list
    run_bg ${BASH_ALIASES[os]} baremetal volume connector list --long '>' "$IRONIC_PATH"/volume_connector_list
    run_bg ${BASH_ALIASES[os]} baremetal volume target list --long '>' "$IRONIC_PATH"/volume_target_list
    run_bg ${BASH_ALIASES[os]} baremetal allocation list --long '>' "$IRONIC_PATH"/allocation_list
    # Driver/Conductor lists are restricted endpoints by default since
    # they provide insight into the overall infrastucture which would
    # be inappropriate in a public cloud context and we don't inherently
    # have the required elevated rights "out of the" box to gather it.
}

# Aodh service gathering - alarms
get_aodh_status() {
    local AODH_PATH="$BASE_COLLECTION_PATH/ctlplane/aodh"
    mkdir -p "$AODH_PATH"
    run_bg ${BASH_ALIASES[os]} alarm list '>' "$AODH_PATH"/alarm_list
}

# Ceilometer, sg-core, prometheus service gathering - metrics
get_ceilometer_status() {
    local CEILOMETER_PATH="$BASE_COLLECTION_PATH/ctlplane/ceilometer"
    if /usr/bin/oc -n openstack get metricstorage metric-storage &> /dev/null; then
        # For `openstack metric list` command to work we need ceilometer
        # in the openstack as well as metricstorage deployed
        # on openshift.
        mkdir -p "$CEILOMETER_PATH"
        run_bg ${BASH_ALIASES[os]} metric list --disable-rbac '>' "$CEILOMETER_PATH"/metric_list
    fi
}

# first we gather generic status of the openstack ctlplane
# then we process the existing services (if an associated
# function has been defined)
run_bg get_status "openstack"
# get the list of existing ctlplane services (once) and
# filter the whole list processing only services with an
# associated function
services=$(${BASH_ALIASES[os]} service list -c Name -f value)
for svc in "${OSP_SERVICES[@]}"; do
    [[ "${services[*]}" =~ ${svc} ]] && get_status "$svc"
done

[[ $CALLED -eq 1 ]] && wait_bg
