#!/bin/bash
# Gather SOS reports from the OpenShift nodes that are running OpenStack pods.
# They are stored uncompressed in the must-gather so there is no nested
# compression of sos reports within the must-gather.
#   SOS_SERVICES: comma separated list of services to gather SOS reports from,
#                 empty string skips sos report gathering.  eg: cinder,glance
#                 Defaults to all of them.
#   SOS_ONLY_PLUGINS: list of sos report plugins to use. Empty string to run
#                     them all. Defaults to: block,cifs,crio,devicemapper,
#                     devices,iscsi,lvm2,memory,multipath,nfs,nis,nvme,podman,
#                     process,processor,selinux,scsi,udev,openstack_edpm
#   SOS_DECOMPRESS: bool to disable decompressing sos reports. Set to 0 to disable
#                   or set to 1 to enable. Defaults to 1
#
# TODO: Confirm with GSS the default for SOS_ONLY_PLUGINS

# When called from the shell directly
if [[ -z "$DIR_NAME" ]]; then
    CALLED=1
    DIR_NAME=$( cd -- "$( dirname -- "${BASH_SOURCE[0]}" )" &> /dev/null && pwd )
    source "${DIR_NAME}/common.sh"
fi

###############################################################################
# VARIABLE INITIALIZATION

# If unset use all default services
if [ "${SOS_SERVICES-unset}" = "unset" ]; then
    SOS_SERVICES=( "${OSP_SERVICES[@]}" )

# If list of services is set and empty, nothing to do
elif [[ -z "${SOS_SERVICES[*]}" ]]; then
    echo "Skipping SOS gathering for controller nodes"
    [[ $CALLED -eq 1 ]] && exit 0
    return

# If set, convert to an array
else
    IFS=',' read -r -a SOS_SERVICES <<< "$SOS_SERVICES"
fi

# Default to some plugins if SOS_ONLY_PLUGINS is not set
SOS_ONLY_PLUGINS="${SOS_ONLY_PLUGINS-block,cifs,crio,devicemapper,devices,iscsi,lvm2,memory,multipath,nfs,nis,nvme,podman,process,processor,selinux,scsi,udev,logs,crypto}"
if [[ -n "$SOS_ONLY_PLUGINS" ]]; then
    SOS_LIMIT="--only-plugins $SOS_ONLY_PLUGINS"
fi

TMPDIR=/var/tmp/sos-osp
SUPPORT_TOOLS=${SUPPORT_TOOLS:-"registry.redhat.io/rhel9/support-tools"}

###############################################################################
# SOS GATHERING

gather_node_sos () {
    node="$1"
    echo "Generating SOS Report for ${node}"
    # Can only run 1 must-gather at a time on each host. We remove any existing
    # toolbox container running from previous time with `podman rm`
    #
    # - Current toolbox can ask via stdin if we want to update [1] so we just
    #   update the container beforehand to prevent it from ever asking. In the
    #   next toolbox [2] that may no longer be the case.
    #   [1]: https://github.com/coreos/toolbox/blob/9a7c840fb4881f406287bf29e5f35b6625c7b358/rhcos-toolbox#L37
    #   [2]: https://github.com/coreos/toolbox/issues/60
    # - Use 2 tar files instead of 1 since tar's "-n --concatenate" and "--append" don't support compressed files
    # - Use tar's transform when adding rotated logs from /var/log/pods so "var" is not removed when untaring with
    #   --strip-components
    #   To avoid performance penalty we don't look for the real directory name using:
    #     $(tar --exclude='*/*' -tf "${FILENAME}" | head -n1)
    #   Instead we use a fake podlogs top directory
    # - Ignore errors on tar since it fails if logs are added while doing the tar as well as if a file doesn't exist
    #   (because the glob has no data or because the file was removed by the log rotation mechanism)
    # - Build LOGS env var to pass tar based on existing files, as tar fails when run with glob that produces no files
    oc debug "node/$node" -- chroot /host bash \
      -c "echo 'TOOLBOX_NAME=toolbox-osp' > /root/.toolboxrc ; \
          rm -rf \"${TMPDIR}\" && \
          mkdir -p \"${TMPDIR}\" && \
          sudo podman rm --force toolbox-osp;  \
          sudo --preserve-env podman pull --authfile /var/lib/kubelet/config.json $SUPPORT_TOOLS && \
          toolbox sos report --batch --all-logs $SOS_LIMIT --tmp-dir=\"${TMPDIR}\" && \
          if [[ \"\$(ls /var/log/pods/*/{*.log.*,*/*.log.*} 2>/dev/null)\" != '' ]]; then tar --ignore-failed-read --warning=no-file-changed -cJf \"${TMPDIR}/podlogs.tar.xz\" --transform 's,^,podlogs/,' /var/log/pods/*/{*.log.*,*/*.log.*} || true; fi"

    # shellcheck disable=SC2181
    if [ $? -ne 0 ]; then
        echo "Failed to run sos report on node ${node}, won't retrieve data"
        return 1
    fi

    # Wait until we know for sure the debug pod is gone, this is to try to
    # workaround the following failure:
    #   Error from server (BadRequest): container "container-00" in pod debug is waiting to start: ContainerCreating
    while oc get pod "${node}-debug" 1>/dev/null 2>&1 1>/dev/; do
        sleep 1
    done
    sleep 1

    # Download and optionally decompress the tar.xz file from the remote node into the
    # must-gather directory.
    # Not decompressing at this stage outside of a CI environment would
    # require changes be made to the yank tool
    echo "Retrieving SOS Report for ${node}"
    # Hint the need to use -i when using tar because there are 2 tars in a single file
    sos_file="${SOS_PATH_NODES}/sosreport-$node-UntarWithArg-i.tar.xz"
    download_logs="${SOS_PATH_NODES}/sosreport-$node.tar.xz-download.logs"
    # Add "--loglevel 6" to help debug in case there's a failure
    oc debug --loglevel 6 "node/$node" -- bash -c "cat \"/host${TMPDIR}/\"*.tar.xz" 2> "${download_logs}" 1> "${sos_file}"

    # shellcheck disable=SC2181
    if [ $? -ne 0 ]; then
        echo "Failed to download sosreport-$node.tar.xz not deleting file"
        return 1
    fi

    rm "${download_logs}"

    # if we are decompressing the sos report, remove the original sos archive
    if [[ ${SOS_DECOMPRESS} -eq 1 ]]; then
        mkdir "${SOS_PATH_NODES}/sosreport-$node"
        tar -i --one-top-level="${SOS_PATH_NODES}/sosreport-$node" --strip-components=1 --exclude='*/dev/null' -Jxf "${sos_file}"
        rm "${sos_file}"
        # Ensure write access to the sos reports directories so must-gather rsync doesn't fail
        chmod +w -R "${SOS_PATH_NODES}/sosreport-$node/"
    fi

    sleep 1
    # Delete the tar.xz file from the remote node
    oc debug "node/$node" -- chroot /host bash -c "rm -rf $TMPDIR"
}


###############################################################################
# MAIN
#
dest_svc_path () {
    local svc=$1
    # Some services have the project name in the service label, eg: cinder
    if [[ "${SOS_SERVICES[*]}" == *"${svc}"* ]]; then
        echo -n "${SOS_PATH}/${svc}"
        return
    fi

    # Other services have the component in the service label, eg: nova-api
    for os_svc in "${SOS_SERVICES[@]}"; do
        if [[ "$svc" == "$os_svc"* ]]; then
            echo -n "${SOS_PATH}/${os_svc}"
            return
        fi
    done
}

mkdir -p "${SOS_PATH_NODES}"

# Get list of nodes and service label for each of the OpenStack service pods
# Not using -o jsonpath='{.spec.nodeName}' because it uses space separator
svc_nodes=$(/usr/bin/oc -n ${OSP_NS} get pod -l service --no-headers -o=custom-columns=NODE:.spec.nodeName,SVC:.metadata.labels.service,NAME:.metadata.name)
nodes=''
while read -r node svc name; do
    svc_path=$(dest_svc_path "$svc")
    if [[ -n "$svc_path" ]]; then
        nodes="${nodes}${node}"$'\n'
        mkdir -p "$svc_path"
        sos_dir="${svc_path}/sos-report-${name}"
        [[ ! -e "$sos_dir" ]] && ln -s "../_all_nodes/sosreport-$node" "$sos_dir" 2>/dev/null
    fi
done <<< "$svc_nodes"

# Remove duplicated nodes because they are running multiple services
nodes=$(echo "$nodes" | sort | uniq)
echo "Will retrieve SOS reports from nodes ${nodes//$'\n'/ }"

for node in $nodes; do
    [[ -z "$node" ]] && continue
    # Gather SOS report for the node in background
    # run_bg cannot be used here as that only support invoking external scripts
    # or executables and not functions
    wait_for_bg_slot
    gather_node_sos "$node" &
done

[[ $CALLED -eq 1 ]] && wait_bg
