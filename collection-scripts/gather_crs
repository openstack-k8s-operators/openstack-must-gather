#!/bin/bash

# When called from the shell directly
if [[ -z "$DIR_NAME" ]]; then
    CALLED=1
    DIR_NAME=$( cd -- "$( dirname -- "${BASH_SOURCE[0]}" )" &> /dev/null && pwd )
    source "${DIR_NAME}/common.sh"
fi

# Resource list
crs=()

# Explictly adding rabbit in the grep because it's not part of the openstack
# umbrella and we rely on the upstream rabbitmq-cluster-operator.
# Also adding monitoring.rhobs, because telemetry uses observability-operator
# for deploying a MonitoringStack for storing and scraping metrics.
for i in $(/usr/bin/oc get crd | grep -E "(openstack|rabbitmq|monitoring)\.(org|com|rhobs)" | awk '{print $1}')
do
  crs+=("$i")
done

# explicitly adding Metal3 BareMetalHosts
crs+=("baremetalhosts.metal3.io")

# we use nested loops to nicely output objects partitioned per namespace, kind
echo "Gathering CRs"
for res in "${crs[@]}"; do
  /usr/bin/oc get "${res}" --all-namespaces -o custom-columns=NAME:.metadata.name,NAMESPACE:.metadata.namespace --no-headers 2> /dev/null | \
  while read -r ocresource; do
    ocobject=$(echo "$ocresource" | awk '{print $1}')
    ocproject=$(echo "$ocresource" | awk '{print $2}')
    if [ -n "${ocproject}" ] && [ "${ocproject}" != "<none>" ]; then
      object_collection_path=${BASE_COLLECTION_PATH}/namespaces/${ocproject}/crs/${res}
      mkdir -p "${object_collection_path}"
      run_bg /usr/bin/oc get "${res}" -n "${ocproject}" -o yaml "${ocobject}" '>' "${object_collection_path}/${ocobject}.yaml"
    fi
  done
done

[[ $CALLED -eq 1 ]] && wait_bg
